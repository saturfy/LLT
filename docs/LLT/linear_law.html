<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>LLT.linear_law API documentation</title>
<meta name="description" content="This sub package collects the functions and classes which calculate the linear laws and generate the LLT features." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>LLT.linear_law</code></h1>
</header>
<section id="section-intro">
<p>This sub package collects the functions and classes which calculate the linear laws and generate the LLT features.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;This sub package collects the functions and classes which calculate the linear laws and generate the LLT features. &#34;&#34;&#34;
import numpy as np


def embedding(sample, num_lags, sampling_step=1, base_step=1, lag_step=1):
    &#34;&#34;&#34;
    Takes time series and creates a time delay embedding matrix from it.

    Parameters
    ----------
    sample : numpy.array shape=(lenght of samples,)
        array containing a time series
    num_lags : int
        The number of lagged datapoints corresponding to every base point. num_lags +1 will
        be the size of samples in the dataset created from the time series (lagges points +
        base point) which is also called the depth of the embedding.
    sampling_step : int
        The time series will be resampled with this step size. If it is 2 or 3 every second or
        third point will be taken from the original.
    base_step : int
        The step size which determines the position of the base points. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    lag_step : int
        The step size which determines the position of the lag points. It starts from the base
        points and moves backward in time.
    Returns
    -------
    nump.ndarray
        Time embedding matrix, shape(number of base points, number of lags+1) The time series is converted
        into this matrix where Y[n,k] = y(t = n * delta t - k * delta t). Every row corresponds to
        a slice of the resampled time series which starts at a given base point and contains
        lagged datapoints moving backward in time.

    Notes
    -----
    All of the prodecures are done using the resampled time series. Every selection starts from
    the last point (most recent) of the time series and moves backward.

    STEP is moving one unit in the resampled series, so lag_step=2 means taking every second point
    as lag point. (these are sampling_step * lag_step  steps in the original series)

    &#34;&#34;&#34;
    # safe copy
    y = np.copy(sample)

    # reverse and resample the time series
    y_resampled = y[-1::-sampling_step]

    # base points
    base_p_indexes = np.arange(0, len(y_resampled), 1)[0: -num_lags * lag_step:base_step]
    base_p_num = len(base_p_indexes)

    # --- create embedding matrix ---
    Y = np.zeros((base_p_num, num_lags + 1))

    for i, bp in enumerate(base_p_indexes):
        Y[i, :] = y_resampled[bp:bp + (num_lags + 1) * lag_step:lag_step]

    return Y


def embedding_dataset(samples, num_lags, concatenate=True, sampling_step=1, base_step=1, lag_step=1):
    &#34;&#34;&#34;
    Takes a set of time series and creates one large time delay embedding matrix from them.
    if concatenate is Ture:
    The time embedding matrices of the individual samples are concatenated into a large block
    matrix where the matrixes corresponding to individual samples are augmented by new rows
    containing the next sample. This way the large block matrix is very high and has
    as many columns as many time delay steps were taken. Each columns corresponds to
    the same time delay.
    if concatenate is False:
    The time embedding matrices of individual samples is not conatenated together. The original
    strucuter of the dataset is kept, except the time series samples are switched to their time
    embedding matrices.

    Parameters
    ----------
    samples : numpy.array shape=(number of samples, lenght of samples)
        array containing a set of time series
    num_lags : int
        The number of lagged datapoints corresponding to every base point. num_lags +1 will
        be the size of samples in the dataset created from the time series (lagges points +
        base point) which is also called the lenght of the embedding.
    sampling_step : int
        The time series will be resampled with this step size. If it is 2 or 3 every second or
        third point will be taken from the original.
    base_step : int
        The step size which determines the position of the base points. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    lag_step : int
        The step size which determines the position of the lag points. It starts from the base
        points and moves backward in time.
    Returns
    -------
    numpy.ndarray
    if concatenate is True: shape=(-1,time embedding depth)
    if concatenate is False: shape=(number of samples, -1, time embedding length)

    &#34;&#34;&#34;
    # time delayed dataset
    Y = []
    for sample in samples:
        y = embedding(sample, num_lags=num_lags, sampling_step=sampling_step, base_step=base_step, lag_step=lag_step)
        if concatenate:
            Y.extend(y)
        else:
            Y.append(y)

    return np.array(Y)


class linear_model:
    &#34;&#34;&#34;
    Fits a model of linear law and calculates properties.

    Attributes
    ----------
    learning_set : numpy.ndarray
            shape(number of samples, embedding length)
            The embedding matrix of a dataset. Columns corresponding to time delays and rows
            contain the different time delay slices.
    PCA_eigenvalues : numpy.ndarray
        array of the eigenvalues coming from the PCA decomposition in increasing order.
    PCA_eigenvectors : numpy.ndarray
        array of the eigenvectors corresponding to the PCA decomposition. Vectors are ordered according
        to the value their corresponding eigenvalues.
    linear_law : numpy.ndarray
        Array of the linear law corresponding to the smallest eigenvalue.
    score : float
        Measures how well the linera law fits the data. Lower the better.
        In a case of a perfect fit the linera law maps every line of the
        learning set to zero. In practice mapping is just close to zero.
        Score is: applying the linear law to the learning set and
        taking the square root of the variance of the resulting vector.
    _num_lags : int
        The number of lagged points in embedding generated from the data_set
    _sampling_step : int
        The resampling step applied on the data set
    _base_step : int
        The step size which determines the position of the base points while
        generating embedding from the data set. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    _lag_step : int
        The step size which determines the position of the lag points while
        generating embedding from the datas set. It starts from the base points
        and moves backward in time.
    &#34;&#34;&#34;

    def __init__(self, data_set, num_lags, sampling_step=1, base_step=1, lag_step=1):
        &#34;&#34;&#34;
        Creates a class instance and initializes with the learning set.

        Parameters
        ----------
        data_set : numpy.ndarray
            shape(number of samples, signal length)
            The time series data of the peaks which will be used to determine the linear law.
            This can be considered the learning set.
        num_lags : int
            The number of lagged datapoints corresponding to every base point. num_lags +1 will
            be the size of samples in the dataset created from the time series (lagges points +
            base point) which is also called the lenght of the embedding.
        sampling_step : int
            The time series will be resampled with this step size. If it is 2 or 3 every second or
            third point will be taken from the original.
        base_step : int
            The step size which determines the position of the base points. It starts from the last
            point of the time series and takes every &#39;base_step&#39;th point as a base point.
        lag_step : int
            The step size which determines the position of the lag points. It starts from the base
            points and moves backward in time.

        &#34;&#34;&#34;
        # create a time delay embedding from the dataset
        self.learning_set = embedding_dataset(data_set, num_lags=num_lags,
                                              sampling_step=sampling_step,
                                              base_step=base_step,
                                              lag_step=lag_step)

        # create the attributes
        self.PCA_eigenvalues = None
        self.PCA_eigenvectors = None
        self.linear_law = None
        self.score = None
        self._num_lags = num_lags
        self._sampling_step = sampling_step
        self._base_step = base_step
        self._lag_step = lag_step

    def fit(self):
        &#34;&#34;&#34;
        Calculates the linear model from learning set.

        Parameters
        ----------
        None

        Returns
        -------
        None

        &#34;&#34;&#34;

        # --- Create PCA matrix ---
        PCA_matrix = self.learning_set.T @ self.learning_set / len(self.learning_set[:, 0])

        # --- linear law ---
        # calculating the eigensystem
        eigenvals, eigenvectors = np.linalg.eig(PCA_matrix)
        idx_list = np.argsort(eigenvals)
        self.PCA_eigenvalues = eigenvals[idx_list]
        self.PCA_eigenvectors = [eigenvectors[:, i] for i in idx_list]

        # --- search for imaginary eigenvalues ---
        imag_list = np.where(np.imag(eigenvals) &gt; 0)[0]
        if len(imag_list) != 0:
            raise ValueError(&#39;Numerical precision error, there are complex eigenvalues.&#39;)

        # --- minimal eigenvalue-vector pair ---
        minimal_index = np.argmin(eigenvals)
        w = eigenvectors[:, minimal_index]
        self.linear_law = w

        # --- calculate the average accuracy of the linear law ---
        self.score = np.sqrt((self.learning_set @ self.linear_law).var())

    def feature_transform(self, data_set, normalize=0.95):
        &#34;&#34;&#34;
        Generates features from the data set of using the linear law.
        The original structure of the  dataset will be preserved
        (sample indexes remain valid), but instead of the individual
        samples, the output will contain thecorresponding features.

        Parameters
        ----------
        data_set : numpy.ndarray
            shape(number of samples, signal length)
            The time series data of the peaks which will be used to determine the linear law.
            This can be considered the learning set.
        normalize : float
            Determines the normalization constant for the features.
            if None: no normalization happens
            if float: percentile
            the normalization constant is determined as the given percentile in the abs(dataset).
        &#34;&#34;&#34;
        transformed_data = []
        # --- data set embedding ---
        embeddings = embedding_dataset(data_set,
                                       concatenate=False,
                                       num_lags=self._num_lags,
                                       sampling_step=self._sampling_step,
                                       base_step=self._base_step,
                                       lag_step=self._lag_step)

        # --- normlization constant ---
        if normalize is not None:
            N = np.percentile(np.abs((self.learning_set @ self.linear_law)), normalize*100)

        for embedding in embeddings:
            features = embedding @ self.linear_law

            # --- normalize ---
            if normalize is not None:
                features = features / N

            transformed_data.append(features)

        return np.array(transformed_data)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="LLT.linear_law.embedding"><code class="name flex">
<span>def <span class="ident">embedding</span></span>(<span>sample, num_lags, sampling_step=1, base_step=1, lag_step=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes time series and creates a time delay embedding matrix from it.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>numpy.array shape=(lenght</code> of <code>samples,)</code></dt>
<dd>array containing a time series</dd>
<dt><strong><code>num_lags</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of lagged datapoints corresponding to every base point. num_lags +1 will
be the size of samples in the dataset created from the time series (lagges points +
base point) which is also called the depth of the embedding.</dd>
<dt><strong><code>sampling_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The time series will be resampled with this step size. If it is 2 or 3 every second or
third point will be taken from the original.</dd>
<dt><strong><code>base_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the base points. It starts from the last
point of the time series and takes every 'base_step'th point as a base point.</dd>
<dt><strong><code>lag_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the lag points. It starts from the base
points and moves backward in time.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>nump.ndarray</code></dt>
<dd>Time embedding matrix, shape(number of base points, number of lags+1) The time series is converted
into this matrix where Y[n,k] = y(t = n * delta t - k * delta t). Every row corresponds to
a slice of the resampled time series which starts at a given base point and contains
lagged datapoints moving backward in time.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>All of the prodecures are done using the resampled time series. Every selection starts from
the last point (most recent) of the time series and moves backward.</p>
<p>STEP is moving one unit in the resampled series, so lag_step=2 means taking every second point
as lag point. (these are sampling_step * lag_step
steps in the original series)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embedding(sample, num_lags, sampling_step=1, base_step=1, lag_step=1):
    &#34;&#34;&#34;
    Takes time series and creates a time delay embedding matrix from it.

    Parameters
    ----------
    sample : numpy.array shape=(lenght of samples,)
        array containing a time series
    num_lags : int
        The number of lagged datapoints corresponding to every base point. num_lags +1 will
        be the size of samples in the dataset created from the time series (lagges points +
        base point) which is also called the depth of the embedding.
    sampling_step : int
        The time series will be resampled with this step size. If it is 2 or 3 every second or
        third point will be taken from the original.
    base_step : int
        The step size which determines the position of the base points. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    lag_step : int
        The step size which determines the position of the lag points. It starts from the base
        points and moves backward in time.
    Returns
    -------
    nump.ndarray
        Time embedding matrix, shape(number of base points, number of lags+1) The time series is converted
        into this matrix where Y[n,k] = y(t = n * delta t - k * delta t). Every row corresponds to
        a slice of the resampled time series which starts at a given base point and contains
        lagged datapoints moving backward in time.

    Notes
    -----
    All of the prodecures are done using the resampled time series. Every selection starts from
    the last point (most recent) of the time series and moves backward.

    STEP is moving one unit in the resampled series, so lag_step=2 means taking every second point
    as lag point. (these are sampling_step * lag_step  steps in the original series)

    &#34;&#34;&#34;
    # safe copy
    y = np.copy(sample)

    # reverse and resample the time series
    y_resampled = y[-1::-sampling_step]

    # base points
    base_p_indexes = np.arange(0, len(y_resampled), 1)[0: -num_lags * lag_step:base_step]
    base_p_num = len(base_p_indexes)

    # --- create embedding matrix ---
    Y = np.zeros((base_p_num, num_lags + 1))

    for i, bp in enumerate(base_p_indexes):
        Y[i, :] = y_resampled[bp:bp + (num_lags + 1) * lag_step:lag_step]

    return Y</code></pre>
</details>
</dd>
<dt id="LLT.linear_law.embedding_dataset"><code class="name flex">
<span>def <span class="ident">embedding_dataset</span></span>(<span>samples, num_lags, concatenate=True, sampling_step=1, base_step=1, lag_step=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes a set of time series and creates one large time delay embedding matrix from them.
if concatenate is Ture:
The time embedding matrices of the individual samples are concatenated into a large block
matrix where the matrixes corresponding to individual samples are augmented by new rows
containing the next sample. This way the large block matrix is very high and has
as many columns as many time delay steps were taken. Each columns corresponds to
the same time delay.
if concatenate is False:
The time embedding matrices of individual samples is not conatenated together. The original
strucuter of the dataset is kept, except the time series samples are switched to their time
embedding matrices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>samples</code></strong> :&ensp;<code>numpy.array shape=(number</code> of <code>samples, lenght</code> of <code>samples)</code></dt>
<dd>array containing a set of time series</dd>
<dt><strong><code>num_lags</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of lagged datapoints corresponding to every base point. num_lags +1 will
be the size of samples in the dataset created from the time series (lagges points +
base point) which is also called the lenght of the embedding.</dd>
<dt><strong><code>sampling_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The time series will be resampled with this step size. If it is 2 or 3 every second or
third point will be taken from the original.</dd>
<dt><strong><code>base_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the base points. It starts from the last
point of the time series and takes every 'base_step'th point as a base point.</dd>
<dt><strong><code>lag_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the lag points. It starts from the base
points and moves backward in time.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>&nbsp;</dd>
<dt><code>if concatenate is True: shape=(-1,time embedding depth)</code></dt>
<dd>&nbsp;</dd>
<dt><code>if concatenate is False: shape=(number</code> of <code>samples, -1, time embedding length)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embedding_dataset(samples, num_lags, concatenate=True, sampling_step=1, base_step=1, lag_step=1):
    &#34;&#34;&#34;
    Takes a set of time series and creates one large time delay embedding matrix from them.
    if concatenate is Ture:
    The time embedding matrices of the individual samples are concatenated into a large block
    matrix where the matrixes corresponding to individual samples are augmented by new rows
    containing the next sample. This way the large block matrix is very high and has
    as many columns as many time delay steps were taken. Each columns corresponds to
    the same time delay.
    if concatenate is False:
    The time embedding matrices of individual samples is not conatenated together. The original
    strucuter of the dataset is kept, except the time series samples are switched to their time
    embedding matrices.

    Parameters
    ----------
    samples : numpy.array shape=(number of samples, lenght of samples)
        array containing a set of time series
    num_lags : int
        The number of lagged datapoints corresponding to every base point. num_lags +1 will
        be the size of samples in the dataset created from the time series (lagges points +
        base point) which is also called the lenght of the embedding.
    sampling_step : int
        The time series will be resampled with this step size. If it is 2 or 3 every second or
        third point will be taken from the original.
    base_step : int
        The step size which determines the position of the base points. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    lag_step : int
        The step size which determines the position of the lag points. It starts from the base
        points and moves backward in time.
    Returns
    -------
    numpy.ndarray
    if concatenate is True: shape=(-1,time embedding depth)
    if concatenate is False: shape=(number of samples, -1, time embedding length)

    &#34;&#34;&#34;
    # time delayed dataset
    Y = []
    for sample in samples:
        y = embedding(sample, num_lags=num_lags, sampling_step=sampling_step, base_step=base_step, lag_step=lag_step)
        if concatenate:
            Y.extend(y)
        else:
            Y.append(y)

    return np.array(Y)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="LLT.linear_law.linear_model"><code class="flex name class">
<span>class <span class="ident">linear_model</span></span>
<span>(</span><span>data_set, num_lags, sampling_step=1, base_step=1, lag_step=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits a model of linear law and calculates properties.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>learning_set</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>shape(number of samples, embedding length)
The embedding matrix of a dataset. Columns corresponding to time delays and rows
contain the different time delay slices.</dd>
<dt><strong><code>PCA_eigenvalues</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>array of the eigenvalues coming from the PCA decomposition in increasing order.</dd>
<dt><strong><code>PCA_eigenvectors</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>array of the eigenvectors corresponding to the PCA decomposition. Vectors are ordered according
to the value their corresponding eigenvalues.</dd>
<dt><strong><code>linear_law</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Array of the linear law corresponding to the smallest eigenvalue.</dd>
<dt><strong><code>score</code></strong> :&ensp;<code>float</code></dt>
<dd>Measures how well the linera law fits the data. Lower the better.
In a case of a perfect fit the linera law maps every line of the
learning set to zero. In practice mapping is just close to zero.
Score is: applying the linear law to the learning set and
taking the square root of the variance of the resulting vector.</dd>
<dt><strong><code>_num_lags</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of lagged points in embedding generated from the data_set</dd>
<dt><strong><code>_sampling_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The resampling step applied on the data set</dd>
<dt><strong><code>_base_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the base points while
generating embedding from the data set. It starts from the last
point of the time series and takes every 'base_step'th point as a base point.</dd>
<dt><strong><code>_lag_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the lag points while
generating embedding from the datas set. It starts from the base points
and moves backward in time.</dd>
</dl>
<p>Creates a class instance and initializes with the learning set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_set</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>shape(number of samples, signal length)
The time series data of the peaks which will be used to determine the linear law.
This can be considered the learning set.</dd>
<dt><strong><code>num_lags</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of lagged datapoints corresponding to every base point. num_lags +1 will
be the size of samples in the dataset created from the time series (lagges points +
base point) which is also called the lenght of the embedding.</dd>
<dt><strong><code>sampling_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The time series will be resampled with this step size. If it is 2 or 3 every second or
third point will be taken from the original.</dd>
<dt><strong><code>base_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the base points. It starts from the last
point of the time series and takes every 'base_step'th point as a base point.</dd>
<dt><strong><code>lag_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The step size which determines the position of the lag points. It starts from the base
points and moves backward in time.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class linear_model:
    &#34;&#34;&#34;
    Fits a model of linear law and calculates properties.

    Attributes
    ----------
    learning_set : numpy.ndarray
            shape(number of samples, embedding length)
            The embedding matrix of a dataset. Columns corresponding to time delays and rows
            contain the different time delay slices.
    PCA_eigenvalues : numpy.ndarray
        array of the eigenvalues coming from the PCA decomposition in increasing order.
    PCA_eigenvectors : numpy.ndarray
        array of the eigenvectors corresponding to the PCA decomposition. Vectors are ordered according
        to the value their corresponding eigenvalues.
    linear_law : numpy.ndarray
        Array of the linear law corresponding to the smallest eigenvalue.
    score : float
        Measures how well the linera law fits the data. Lower the better.
        In a case of a perfect fit the linera law maps every line of the
        learning set to zero. In practice mapping is just close to zero.
        Score is: applying the linear law to the learning set and
        taking the square root of the variance of the resulting vector.
    _num_lags : int
        The number of lagged points in embedding generated from the data_set
    _sampling_step : int
        The resampling step applied on the data set
    _base_step : int
        The step size which determines the position of the base points while
        generating embedding from the data set. It starts from the last
        point of the time series and takes every &#39;base_step&#39;th point as a base point.
    _lag_step : int
        The step size which determines the position of the lag points while
        generating embedding from the datas set. It starts from the base points
        and moves backward in time.
    &#34;&#34;&#34;

    def __init__(self, data_set, num_lags, sampling_step=1, base_step=1, lag_step=1):
        &#34;&#34;&#34;
        Creates a class instance and initializes with the learning set.

        Parameters
        ----------
        data_set : numpy.ndarray
            shape(number of samples, signal length)
            The time series data of the peaks which will be used to determine the linear law.
            This can be considered the learning set.
        num_lags : int
            The number of lagged datapoints corresponding to every base point. num_lags +1 will
            be the size of samples in the dataset created from the time series (lagges points +
            base point) which is also called the lenght of the embedding.
        sampling_step : int
            The time series will be resampled with this step size. If it is 2 or 3 every second or
            third point will be taken from the original.
        base_step : int
            The step size which determines the position of the base points. It starts from the last
            point of the time series and takes every &#39;base_step&#39;th point as a base point.
        lag_step : int
            The step size which determines the position of the lag points. It starts from the base
            points and moves backward in time.

        &#34;&#34;&#34;
        # create a time delay embedding from the dataset
        self.learning_set = embedding_dataset(data_set, num_lags=num_lags,
                                              sampling_step=sampling_step,
                                              base_step=base_step,
                                              lag_step=lag_step)

        # create the attributes
        self.PCA_eigenvalues = None
        self.PCA_eigenvectors = None
        self.linear_law = None
        self.score = None
        self._num_lags = num_lags
        self._sampling_step = sampling_step
        self._base_step = base_step
        self._lag_step = lag_step

    def fit(self):
        &#34;&#34;&#34;
        Calculates the linear model from learning set.

        Parameters
        ----------
        None

        Returns
        -------
        None

        &#34;&#34;&#34;

        # --- Create PCA matrix ---
        PCA_matrix = self.learning_set.T @ self.learning_set / len(self.learning_set[:, 0])

        # --- linear law ---
        # calculating the eigensystem
        eigenvals, eigenvectors = np.linalg.eig(PCA_matrix)
        idx_list = np.argsort(eigenvals)
        self.PCA_eigenvalues = eigenvals[idx_list]
        self.PCA_eigenvectors = [eigenvectors[:, i] for i in idx_list]

        # --- search for imaginary eigenvalues ---
        imag_list = np.where(np.imag(eigenvals) &gt; 0)[0]
        if len(imag_list) != 0:
            raise ValueError(&#39;Numerical precision error, there are complex eigenvalues.&#39;)

        # --- minimal eigenvalue-vector pair ---
        minimal_index = np.argmin(eigenvals)
        w = eigenvectors[:, minimal_index]
        self.linear_law = w

        # --- calculate the average accuracy of the linear law ---
        self.score = np.sqrt((self.learning_set @ self.linear_law).var())

    def feature_transform(self, data_set, normalize=0.95):
        &#34;&#34;&#34;
        Generates features from the data set of using the linear law.
        The original structure of the  dataset will be preserved
        (sample indexes remain valid), but instead of the individual
        samples, the output will contain thecorresponding features.

        Parameters
        ----------
        data_set : numpy.ndarray
            shape(number of samples, signal length)
            The time series data of the peaks which will be used to determine the linear law.
            This can be considered the learning set.
        normalize : float
            Determines the normalization constant for the features.
            if None: no normalization happens
            if float: percentile
            the normalization constant is determined as the given percentile in the abs(dataset).
        &#34;&#34;&#34;
        transformed_data = []
        # --- data set embedding ---
        embeddings = embedding_dataset(data_set,
                                       concatenate=False,
                                       num_lags=self._num_lags,
                                       sampling_step=self._sampling_step,
                                       base_step=self._base_step,
                                       lag_step=self._lag_step)

        # --- normlization constant ---
        if normalize is not None:
            N = np.percentile(np.abs((self.learning_set @ self.linear_law)), normalize*100)

        for embedding in embeddings:
            features = embedding @ self.linear_law

            # --- normalize ---
            if normalize is not None:
                features = features / N

            transformed_data.append(features)

        return np.array(transformed_data)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="LLT.linear_law.linear_model.feature_transform"><code class="name flex">
<span>def <span class="ident">feature_transform</span></span>(<span>self, data_set, normalize=0.95)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates features from the data set of using the linear law.
The original structure of the
dataset will be preserved
(sample indexes remain valid), but instead of the individual
samples, the output will contain thecorresponding features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_set</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>shape(number of samples, signal length)
The time series data of the peaks which will be used to determine the linear law.
This can be considered the learning set.</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>float</code></dt>
<dd>Determines the normalization constant for the features.
if None: no normalization happens
if float: percentile
the normalization constant is determined as the given percentile in the abs(dataset).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_transform(self, data_set, normalize=0.95):
    &#34;&#34;&#34;
    Generates features from the data set of using the linear law.
    The original structure of the  dataset will be preserved
    (sample indexes remain valid), but instead of the individual
    samples, the output will contain thecorresponding features.

    Parameters
    ----------
    data_set : numpy.ndarray
        shape(number of samples, signal length)
        The time series data of the peaks which will be used to determine the linear law.
        This can be considered the learning set.
    normalize : float
        Determines the normalization constant for the features.
        if None: no normalization happens
        if float: percentile
        the normalization constant is determined as the given percentile in the abs(dataset).
    &#34;&#34;&#34;
    transformed_data = []
    # --- data set embedding ---
    embeddings = embedding_dataset(data_set,
                                   concatenate=False,
                                   num_lags=self._num_lags,
                                   sampling_step=self._sampling_step,
                                   base_step=self._base_step,
                                   lag_step=self._lag_step)

    # --- normlization constant ---
    if normalize is not None:
        N = np.percentile(np.abs((self.learning_set @ self.linear_law)), normalize*100)

    for embedding in embeddings:
        features = embedding @ self.linear_law

        # --- normalize ---
        if normalize is not None:
            features = features / N

        transformed_data.append(features)

    return np.array(transformed_data)</code></pre>
</details>
</dd>
<dt id="LLT.linear_law.linear_model.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the linear model from learning set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self):
    &#34;&#34;&#34;
    Calculates the linear model from learning set.

    Parameters
    ----------
    None

    Returns
    -------
    None

    &#34;&#34;&#34;

    # --- Create PCA matrix ---
    PCA_matrix = self.learning_set.T @ self.learning_set / len(self.learning_set[:, 0])

    # --- linear law ---
    # calculating the eigensystem
    eigenvals, eigenvectors = np.linalg.eig(PCA_matrix)
    idx_list = np.argsort(eigenvals)
    self.PCA_eigenvalues = eigenvals[idx_list]
    self.PCA_eigenvectors = [eigenvectors[:, i] for i in idx_list]

    # --- search for imaginary eigenvalues ---
    imag_list = np.where(np.imag(eigenvals) &gt; 0)[0]
    if len(imag_list) != 0:
        raise ValueError(&#39;Numerical precision error, there are complex eigenvalues.&#39;)

    # --- minimal eigenvalue-vector pair ---
    minimal_index = np.argmin(eigenvals)
    w = eigenvectors[:, minimal_index]
    self.linear_law = w

    # --- calculate the average accuracy of the linear law ---
    self.score = np.sqrt((self.learning_set @ self.linear_law).var())</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="LLT" href="index.html">LLT</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="LLT.linear_law.embedding" href="#LLT.linear_law.embedding">embedding</a></code></li>
<li><code><a title="LLT.linear_law.embedding_dataset" href="#LLT.linear_law.embedding_dataset">embedding_dataset</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="LLT.linear_law.linear_model" href="#LLT.linear_law.linear_model">linear_model</a></code></h4>
<ul class="">
<li><code><a title="LLT.linear_law.linear_model.feature_transform" href="#LLT.linear_law.linear_model.feature_transform">feature_transform</a></code></li>
<li><code><a title="LLT.linear_law.linear_model.fit" href="#LLT.linear_law.linear_model.fit">fit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>